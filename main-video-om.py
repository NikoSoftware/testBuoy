import cv2
import numpy as np
from ais_bench.infer.interface import InferSession
import time  # æ·»åŠ æ—¶é—´æ¨¡å—
import sys  # ç”¨äºè¿›åº¦æ¡åˆ·æ–°


class YOLOv11_NPU_Inference:
    def __init__(self, model_path):
        # åˆå§‹åŒ–NPUæ¨ç†ä¼šè¯
        self.session = InferSession(device_id=0, model_path=model_path)
        self.input_shape = (640, 640)  # YOLOæ ‡å‡†è¾“å…¥å°ºå¯¸
        self.output_shape = (1, 6, 8400)  # æŒ‡å®šè¾“å‡ºå¼ é‡å½¢çŠ¶
        self.total_inference_time = 0  # æ€»æ¨ç†æ—¶é—´ç»Ÿè®¡
        self.total_frames = 0  # æ€»å¸§æ•°ç»Ÿè®¡
        self.class_names = self.load_class_names()  # åŠ è½½ç±»åˆ«åç§°

    def load_class_names(self):
        """åŠ è½½ç±»åˆ«åç§°ï¼ˆç¤ºä¾‹ï¼‰"""
        return [
            'cat', 'dog'
        ]

    def preprocess(self, frame):
        """å›¾åƒé¢„å¤„ç†ï¼šç¼©æ”¾åˆ°è¾“å…¥å°ºå¯¸å¹¶å½’ä¸€åŒ–"""
        # ä¿æŒå®½é«˜æ¯”çš„ç¼©æ”¾
        h, w = frame.shape[:2]
        scale = min(self.input_shape[1] / w, self.input_shape[0] / h)
        new_w, new_h = int(w * scale), int(h * scale)
        resized = cv2.resize(frame, (new_w, new_h))

        # åˆ›å»ºå¡«å……åçš„ç”»å¸ƒ
        canvas = np.full((self.input_shape[0], self.input_shape[1], 3), 114, dtype=np.uint8)
        top = (self.input_shape[0] - new_h) // 2
        left = (self.input_shape[1] - new_w) // 2
        canvas[top:top + new_h, left:left + new_w] = resized

        # è½¬æ¢ä¸ºRGBã€å½’ä¸€åŒ–ã€è°ƒæ•´ç»´åº¦é¡ºåº
        canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)
        blob = canvas.astype(np.float32) / 255.0
        blob = blob.transpose(2, 0, 1)[np.newaxis]  # HWC -> NCHW
        return blob, (left, top, scale), (h, w)

    def postprocess(self, outputs, meta, conf_thres=0.5):
        """è§£æNPUè¾“å‡ºï¼šè¿‡æ»¤æ£€æµ‹ç»“æœå¹¶æ˜ å°„å›åŸå›¾åæ ‡"""
        left, top, scale = meta["padding"]
        original_h, original_w = meta["original_shape"]

        # æå–é¢„æµ‹ç»“æœ [1,6,8400] -> [8400,6]
        predictions = outputs[0].squeeze(0).T
        detections = []

        for pred in predictions:
            conf = pred[4]
            if conf < conf_thres:
                continue

            # è§£æè¾¹ç•Œæ¡† (cxcywhæ ¼å¼)
            cx, cy, w, h = pred[:4] * 2 - 0.5
            x1 = int((cx - w / 2 - left) / scale)
            y1 = int((cy - h / 2 - top) / scale)
            x2 = int((cx + w / 2 - left) / scale)
            y2 = int((cy + h / 2 - top) / scale)

            # è£å‰ªåˆ°å›¾åƒèŒƒå›´å†…
            x1 = max(0, min(x1, original_w))
            y1 = max(0, min(y1, original_h))
            x2 = max(0, min(x2, original_w))
            y2 = max(0, min(y2, original_h))

            # æ·»åŠ ç»“æœ (x1, y1, x2, y2, conf, cls)
            cls_id = int(pred[5])
            detections.append([x1, y1, x2, y2, conf, cls_id])

        return detections

    def run_video(self, video_path, output_path="./output_dog.mp4", show_progress=True):
        """è§†é¢‘æµæ¨ç†ä¸»å¾ªç¯ï¼Œæ·»åŠ æ—¶é—´ç»Ÿè®¡å’Œç›®æ ‡åæ ‡è¾“å‡º"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Cannot open video {video_path}")

        # è·å–è§†é¢‘å±æ€§å¹¶åˆå§‹åŒ–è¾“å‡º
        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))

        print(f"ğŸ“º å¼€å§‹è§†é¢‘å¤„ç†: {video_path}")
        print(f"  - åˆ†è¾¨ç‡: {w}x{h}, FPS: {fps:.1f}, æ€»å¸§æ•°: {total_frames}")

        if total_frames <= 0:
            print("âš ï¸ è­¦å‘Š: æ— æ³•ç¡®å®šè§†é¢‘æ€»å¸§æ•°ï¼Œè¿›åº¦æ˜¾ç¤ºå°†å—é™")

        # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡
        frame_counter = 0
        processing_times = []
        start_time = time.time()
        prev_frame_time = time.time()  # ç”¨äºè®¡ç®—å¸§ç‡

        # å¸§å¤„ç†å¾ªç¯
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_counter += 1
            frame_start = time.time()

            # é¢„å¤„ç†
            preprocess_start = time.time()
            blob, padding_info, orig_shape = self.preprocess(frame)
            preprocess_time = time.time() - preprocess_start

            # NPUæ¨ç†
            inference_start = time.time()
            outputs = self.session.infer([blob])  # åŒæ­¥æ¨ç†
            inference_time = time.time() - inference_start
            self.total_inference_time += inference_time

            # åå¤„ç†
            postprocess_start = time.time()
            dets = self.postprocess(outputs, {
                "padding": padding_info,
                "original_shape": orig_shape
            })
            postprocess_time = time.time() - postprocess_start

            # å¤„ç†æ€»æ—¶é—´
            total_frame_time = time.time() - frame_start
            processing_times.append(total_frame_time)

            # è®¡ç®—å®æ—¶FPS
            current_time = time.time()
            fps = 1.0 / (current_time - prev_frame_time)
            prev_frame_time = current_time

            # æ‰“å°æ£€æµ‹ç»“æœå’Œåæ ‡
            if dets:
                print(f"\nğŸŸ¢ å¸§ #{frame_counter} æ£€æµ‹åˆ° {len(dets)} ä¸ªç›®æ ‡:")
                for i, (x1, y1, x2, y2, conf, cls_id) in enumerate(dets):
                    class_name = self.class_names[cls_id] if cls_id < len(self.class_names) else f"Class_{cls_id}"
                    print(f"  ç›®æ ‡ {i + 1}: {class_name} (ç½®ä¿¡åº¦: {conf:.2f})")
                    print(f"      åæ ‡: ({x1}, {y1}) - ({x2}, {y2})")
                    print(f"      å°ºå¯¸: {x2 - x1}x{y2 - y1} åƒç´ ")

            # åœ¨ç”»é¢ä¸Šç»˜åˆ¶FPSå’Œç›®æ ‡æ•°é‡
            cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            cv2.putText(frame, f"Targets: {len(dets)}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

            # ç»˜åˆ¶æ£€æµ‹æ¡†å¹¶è¾“å‡º
            for x1, y1, x2, y2, conf, cls_id in dets:
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame, f"{cls_id}:{conf:.2f}",
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.9, (36, 255, 12), 2)

            out.write(frame)

            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            if show_progress:
                if total_frames > 0:
                    progress = frame_counter / total_frames * 100
                    print(f"\rğŸ”„ å¤„ç†è¿›åº¦: {frame_counter}/{total_frames} ({progress:.1f}%) | "
                          f"æ¨ç†: {inference_time * 1000:.1f}ms | "
                          f"æ€»å¸§æ—¶: {total_frame_time * 1000:.1f}ms", end="")
                else:
                    print(f"\rğŸ”„ å¤„ç†å¸§æ•°: {frame_counter} | "
                          f"æ¨ç†: {inference_time * 1000:.1f}ms | "
                          f"æ€»å¸§æ—¶: {total_frame_time * 1000:.1f}ms", end="")

        # å¤„ç†å®Œæˆ
        total_elapsed = time.time() - start_time

        # æ€§èƒ½æ‘˜è¦
        print("\n\nâœ… å¤„ç†å®Œæˆ!")
        print(f"  - å¤„ç†æ€»æ—¶é—´: {total_elapsed:.2f}ç§’")
        print(f"  - å¹³å‡å¸§ç‡: {frame_counter / total_elapsed:.1f}FPS")

        # å„é˜¶æ®µè€—æ—¶åˆ†æ
        if processing_times:
            avg_prep = sum([t * 1000 for t in processing_times]) / frame_counter
            print(f"  - å¹³å‡å•å¸§è€—æ—¶: {avg_prep:.1f}ms")

        # NPUæ€§èƒ½ç»Ÿè®¡
        print(f"\nğŸ”¥ NPUåˆ©ç”¨ç‡ç»Ÿè®¡:")
        print(f"  - æ€»æ¨ç†æ—¶é—´: {self.total_inference_time:.2f}ç§’")
        print(f"  - æ€»å¤„ç†å¸§æ•°: {frame_counter}å¸§")
        print(f"  - å¹³å‡æ¨ç†é€Ÿåº¦: {self.total_inference_time / frame_counter * 1000:.1f}ms/å¸§")

        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()
        cv2.destroyAllWindows()

        print(f"\nğŸ’¾ è¾“å‡ºè§†é¢‘å·²ä¿å­˜è‡³: {output_path}")


if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    model_path = "./runs/train/train/weights/best.om"
    video_path = "./datasets/video/dog.mp4"
    output_video = "./output_dog.mp4"

    # åˆå§‹åŒ–å¹¶è¿è¡Œ
    detector = YOLOv11_NPU_Inference(model_path)

    # æ·»åŠ è¶…æ—¶æ—¶é—´æ‰“å°ï¼ˆé˜²æ­¢é•¿æ—¶é—´æ— å“åº”ï¼‰
    print("â±ï¸ å¼€å§‹æ‰§è¡Œ...")
    start_time = time.time()

    try:
        detector.run_video(video_path, output_path=output_video)
    finally:
        # æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
        total_time = time.time() - start_time
        print(f"\nğŸ•’ ç¨‹åºæ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")